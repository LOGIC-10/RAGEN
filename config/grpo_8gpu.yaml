defaults:
  - ppo_trainer # 导入 ppo_trainer 的默认配置，通常指向 verl/verl/trainer/config/ppo_trainer.yaml
  - _self_      # 表示引用当前文件自身的配置作为默认值的一部分
  - envs        # 导入 envs.yaml 中定义的环境配置

system:
  CUDA_VISIBLE_DEVICES: "0,1,2,3,4,5,6,7" # 指定训练和推理时可见的 GPU 设备 ID

micro_batch_size_per_gpu: 1       # 每个 GPU 上的微批次大小，用于控制单次前向/后向传播的数据量
ppo_mini_batch_size: 8           # PPO/GRPO 算法中用于更新策略的 mini-batch 大小，确保断言通过
model_path: /map-vepfs/qinyu/CodeSpace/LLaMA-Factory/saves/qwen2_5-0_5b/full/sft # 预训练模型的路径
enable_response_mask: True        # 是否启用响应掩码。启用后，在计算损失时可能不再计算 P(st|history)，有助于提高 rollout/old_log_prob 的稳定性
grpo_advantage_length_weight: False # 是否在 GRPO (Generalized Reward Policy Optimization) 优势估计中对长度进行加权。如果 critic/advantage_estimator 是 GRPO 且 critic/advantages/mean 过低，可以尝试启用此项以鼓励更长的推理链并防止模型输出坍塌

lora: # LoRA (Low-Rank Adaptation) 微调相关参数
  rank: 0                         # LoRA 的秩。如果为 0，则不使用 LoRA
  alpha: 16                       # LoRA 的 alpha 参数，用于缩放 LoRA 权重
  target_modules: all-linear      # LoRA 应用的目标模块，'all-linear' 表示所有线性层

actor_rollout_ref: # Actor (策略网络)、Rollout (数据收集) 和 Reference (参考模型) 相关配置
  model: # Actor 和 Ref 模型共享的基础模型配置
    path: ${model_path}             # 模型路径，引用顶层的 model_path
    lora_rank: ${lora.rank}         # LoRA 秩，引用顶层的 lora.rank
    lora_alpha: ${lora.alpha}       # LoRA alpha，引用顶层的 lora.alpha
    target_modules: ${lora.target_modules} # LoRA 目标模块，引用顶层的 lora.target_modules
  actor: # Actor (策略网络) 特定配置
    ppo_mini_batch_size: ${ppo_mini_batch_size}  # PPO mini-batch 大小，引用顶层的 ppo_mini_batch_size
    micro_batch_size_per_gpu: ${micro_batch_size_per_gpu} # 每个 GPU 的微批次大小，引用顶层的 micro_batch_size_per_gpu
    ppo_micro_batch_size_per_gpu: ${micro_batch_size_per_gpu} # PPO 训练时每个 GPU 的微批次大小，引用顶层的 micro_batch_size_per_gpu
    use_ref: True                   # 是否使用参考模型 (Reference Model) 来计算 KL 散度或作为 PPO 的旧策略
    use_dynamic_bsz: True           # 启用动态 batch size，根据显存自动调整，缓解OOM
    entropy_coeff: 0.001            # 熵正则化系数，鼓励策略探索
    use_kl_loss: False              # 是否使用 KL 散度损失项
    kl_loss_coef: 0.000             # KL 散度损失的系数
    kl_loss_type: kl                # KL 散度损失的类型 (如 'kl', 'abs', 'mse')
    clip_ratio_low: 0.2             # PPO 裁剪目标的下限比例
    clip_ratio_high: 0.28           # PPO 裁剪目标的上限比例
    grpo_advantage_length_weight: ${grpo_advantage_length_weight} # 是否对 GRPO 优势估计进行长度加权，引用顶层同名参数
    fsdp_config:                    # FSDP 相关配置，开启 CPU 卸载显存
      param_offload: True           # 参数卸载到CPU，显著降低GPU显存占用
      optimizer_offload: True       # 优化器状态卸载到CPU
    optim: # 优化器配置
      betas: [0.9, 0.999]           # AdamW 或类似优化器的 beta 参数
    ppo_max_token_len_per_gpu: 8192 # 从 16384 增加到 32768，以支持更长的序列
  ref: # Reference (参考模型) 特定配置
    log_prob_micro_batch_size_per_gpu: ${micro_batch_size_per_gpu} # 计算参考模型 log_prob 时每个 GPU 的微批次大小
  rollout: # Rollout (数据收集/采样) 配置
    log_prob_micro_batch_size_per_gpu: ${micro_batch_size_per_gpu} # Rollout 时计算 log_prob 的微批次大小
    tensor_model_parallel_size: 2   # 模型张量并行的 GPU 数量，改为2以匹配Qwen2-0.5B的14个注意力头
    max_model_len: 8192            # 增加模型支持的最大序列长度 (prompt + response)
    prompt_length: 1                # 提示 (prompt) 的长度，这里标记为 'useless'，可能实际未使用或被其他参数覆盖
    response_length: 512            # 降低响应长度至512，显著减小显存占用
    gpu_memory_utilization: 0.8     # 目标 GPU 显存利用率 (vLLM 等推理引擎参数)
    max_num_batched_tokens: 8192   # 增加批处理的最大 token 数，与 max_model_len 保持一致
    temperature: 1                  #采样温度，控制生成文本的随机性，较高温度更随机
    rollout_filter_ratio: 1      # Rollout 过滤器比例，可能用于过滤掉低质量的轨迹
    rollout_filter_type: std        # Rollout 过滤器类型 ('max_mean' 或 'std')
    enforce_eager: True             # 是否强制使用 Eager模式 (PyTorch)。对于小模型，与 free_cache_engine 一同设为 False 可能加速 rollout
    free_cache_engine: True         # 是否在 rollout 后释放缓存引擎 (vLLM 参数)
    val_kwargs: # 验证集 rollout 的特定参数
      do_sample: True               # 验证时是否进行采样
      temperature: 0.5              # 验证时的采样温度
    tp_size_check: false             # 是否检查张量并行大小 (tensor_model_parallel_size) 的有效性

critic: # Critic (价值网络) 相关配置
  ppo_mini_batch_size: ${ppo_mini_batch_size} # PPO mini-batch 大小，引用顶层的 ppo_mini_batch_size
  ppo_micro_batch_size_per_gpu: ${micro_batch_size_per_gpu} # PPO 训练时每个 GPU 的微批次大小
  model: # Critic 模型配置
    path: ${model_path}             # 模型路径
    lora_rank: ${lora.rank}         # LoRA 秩
    lora_alpha: ${lora.alpha}       # LoRA alpha
    target_modules: ${lora.target_modules} # LoRA 目标模块
    fsdp_config:
      param_offload: True          # 同步开启参数卸载，减少显存
      optimizer_offload: True
      offload_policy: False # only for fsdp2, offload param\grad\optimizer during train
  optim: # Critic 优化器配置
    betas: [0.9, 0.999]           # AdamW 或类似优化器的 beta 参数

data: # 数据相关配置
  max_prompt_length: null         # 最大提示长度，null 表示不限制或由其他配置决定
  max_response_length: null       # 最大响应长度，null 表示不限制或由其他配置决定
  train_batch_size: null          # 训练批次大小，null 表示不限制或由其他配置决定 (通常是 micro_batch_size_per_gpu * n_gpus_per_node * gradient_accumulation_steps)

algorithm: # 强化学习算法核心参数
  gamma: 1.0                      # 折扣因子 (discount factor)
  lam: 1.0                        # GAE (Generalized Advantage Estimation) 中的 lambda 参数
  high_level_gamma: 0.95          # (如果使用) 高层级策略的折扣因子
  adv_estimator: grpo             # 优势函数估计方法，改为GRPO
  norm_adv_by_std_in_grpo: True   # 启用GRPO标准差归一化
  bi_level_gae: False             # 关闭双层GAE
  kl_penalty: kl                  # KL 散度惩罚的估计方法
  kl_ctrl: # KL 控制器配置，用于动态调整 KL 散度惩罚系数
    type: fixed                   # KL 控制器类型，'fixed' 表示固定系数
    kl_coef: 0.000                # 固定的 KL 散度惩罚系数

trainer: # 训练器 (Trainer) 配置
  project_name: ragen_latest      # WandB (Weights & Biases) 项目名称
  experiment_name: grpo_8gpu      # WandB 实验名称，标记为GRPO 8卡
  total_training_steps: 10000     # 总训练步数，设置为较大值以进行更长时间的训练。由于使用了cycle迭代器，数据会自动循环使用
  validation_steps: 1             # 验证步数。验证实例数 = validation_steps * val_env_groups * group_size
  val_before_train: True          # 是否在训练开始前进行一次验证
  n_gpus_per_node: 8              # 每个节点的 GPU 数量，和CUDA_VISIBLE_DEVICES一致
  test_freq: 10                  # 测试频率 (每多少个训练步进行一次测试)
  save_freq: 10                  # 每500步保存一次checkpoint，避免保存太频繁
  default_local_dir: checkpoints/${trainer.project_name}/${trainer.experiment_name}  # checkpoint保存路径，格式为：checkpoints/ragen_latest/grpo_8gpu/
  max_actor_ckpt_to_keep: 5       # 最多保留5个actor的checkpoint，避免占用太多磁盘空间
  max_critic_ckpt_to_keep: 5      # 最多保留5个critic的checkpoint，避免占用太多磁盘空间
  generations_to_log_to_wandb: # 记录到 WandB 的生成样本数量
    train: 256                    # 训练时记录的样本数
    val: 50                       # 验证时记录的样本数
  logger: [ 'console', 'wandb' ]  # 日志记录方式，'console' 表示控制台输出，'wandb' 表示记录到 WandB

agent_proxy: # Agent 代理配置，用于处理 Agent 与环境的交互逻辑
  max_turn: 5                     # Agent 在一个 episode 中最大交互轮数
  action_sep: "||"                # 多个离散动作之间的分隔符
  max_actions_per_turn: 5         # Agent 单轮最大输出动作数量
  use_turn_scores: False          # 是否使用轮次得分。对于 GAE，当将 token 级奖励应用于 token 级优势时很重要。如果为 False，则将最后一轮的得分总和作为该轮的奖励。
  enable_think: True              # 是否启用 "思考" 步骤 (如 CoT)。False 表示不进行思考的 RL
  reward_normalization: # 奖励归一化配置
    grouping: "state"             # 归一化分组方式 ('state', 'batch', 'inductive')
    method: "mean_std"            # 归一化方法 ('asym_clip', 'identity', 'mean_std')，GRPO推荐mean_std

es_manager: # Environment State Manager (环境状态管理器) 配置
  format_penalty: -0.1            # 对格式错误的 Agent 输出施加的惩罚值
  train: # 训练环境配置
    env_groups: 8                 # 训练时环境组的数量。每个组内的环境配置和种子在每代中是相同的。
    group_size: 2                 # 每个环境组的大小 (并行的环境实例数量)，减小以避免OOM
    env_configs: # 环境的具体配置
      tags: ["DeepResearch"]      # 使用的环境标签，对应 `envs.yaml` 中的环境定义
      n_groups: [8]               # 每个标签的环境分配到的组数
  val: # 验证环境配置
    env_groups: 8                 # 验证时环境组的数量
    group_size: 4                 # 验证时每个环境组的大小。应设为4，保证组内有统计意义
    env_configs:
      tags: ["DeepResearch"]
      n_groups: [8]

ctx_manager: # Context Manager (上下文管理器) 配置，可能用于管理生成过程中的上下文
  generation: # 生成配置 (可能传递给 vLLM 或类似推理服务)
    gen_config:
      response_length: ${actor_rollout_ref.rollout.response_length} # 生成响应的长度
      temperature: ${actor_rollout_ref.rollout.temperature}        # 生成温度
      top_p: ${actor_rollout_ref.rollout.top_p}                    # top-p (nucleus) 采样参数 (这里 base.yaml 中未直接定义 top_p，可能从 ppo_trainer 默认值或其他地方继承)
      top_k: ${actor_rollout_ref.rollout.top_k}                    # top-k 采样参数 (这里 base.yaml 中未直接定义 top_k)
      kwargs: null                                                 # 其他传递给生成函数的参数 




# wandb登陆: export WANDB_API_KEY=e4f35ea9fc4cb2c76c5eeb868bb52f8db8f565bd